{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"g.png\" height=\"300\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style =\"color : #D36C34 ; font-size: 50px; font-weight:900;  text-align: center;text-decoration: underline;\"> Distance de mahalanobis  </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style =\"color : #8080FF ; font-size: 25px; font-weight:700\"> C'est quoi? </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__La distance de Mahalanobis est une mesure de la distance entre un point P et une distribution D , introduite par P. C. Mahalanobis en 1936. Il s'agit d'une généralisation multidimensionnelle de l'idée de mesurer le nombre d' écarts types loin de P de la moyenne de D . Cette distance est nulle pour P à la moyenne de D et augmente à mesure que P s'éloigne de la moyenne le long de chaque axe de composante principale . Si chacun de ces axes est mis à l'échelle pour avoir une variance unitaire, alors la distance de Mahalanobis correspond à la normeDistance euclidienne dans l'espace transformé. La distance de Mahalanobis est donc sans unité , invariante d'échelle et prend en compte les corrélations de l' ensemble de données.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ddm.png\" height=\"300\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style =\"color : #8080FF ; font-size: 25px; font-weight:700\"> Exemple </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"expl.png\" height=\"300\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style =\"color : #8080FF ; font-size: 25px; font-weight:700\"> Cas Particuliers </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__-Si la matrice de covariance est la matrice d'identité, la distance Mahalanobis est réduite à distance euclidienne.__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__-Si la matrice de covariance est diagonale, la mesure de distance résultante est appelée distance euclidienne normalisée.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style =\"color : #8080FF ; font-size: 25px; font-weight:700\"> Applications </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__La distance Mahalanobis est nécessaire du problème d'identification des crânes sur la base des mesures en 1927.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__La distance Mahalanobis est largement utilisé dans les problèmes regroupement et d'autres techniques classification statistique. Elle est étroitement liée à Au hasard Variable T-carré de Hotelling, utilisé pour les tests à plusieurs variables et statistiques analyse discriminante Fisher, qui sont utilisés dans les problèmes apprentissage supervisé.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Distance de Mahalanobis est également utilisé pour révéler valeurs aberrantes, en particulier dans le développement de modèles régression linéaire. Un point qui a une grande distance de Mahalanobis à partir du reste des échantillons a une grande influence sur la pente ou sur l'équation des coefficients de régression.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style =\"color : #8080FF ; font-size: 25px; font-weight:700\"> Cas d'utilisation de la distance de mahalanobis </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"utilisation.png\" height=\"300\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.random.normal(0, 5, 1000)\n",
    "x2 = np.random.normal(0, 1, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'x1': x1, \n",
    "    'x2': x2\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "sns.scatterplot(x='x1', y='x2', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mahalanobis_distance(p, q, df):\n",
    "    A = p - q\n",
    "    B = (p - q).T\n",
    "    covariance_matrix = np.array(df.cov())\n",
    "    inverse_covariance_matrix = np.linalg.inv(covariance_matrix)\n",
    "    return np.dot(np.dot(A, inverse_covariance_matrix), B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(p, q):\n",
    "    return np.sqrt(np.sum((p - q) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x1_min_arg = x1[np.argsort(x1)[0]]\n",
    "x1_max_arg = x1[np.argsort(x1)[-1]]\n",
    "x2_min_arg, x2_max_arg = x2[np.where((x2 > np.mean(x2) - .2) & (x2 < np.mean(x2) + .2))[0][0:2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "sns.scatterplot(x='x1', y='x2', data=df)\n",
    "sns.scatterplot(x=[x1_min_arg, x1_max_arg], y=[x2_min_arg, x2_max_arg], color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.array([[x1_min_arg, x2_min_arg]])\n",
    "q = np.array([[x1_max_arg, x2_max_arg]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidean_distance(p, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mahalanobis_distance(p, q, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'https://raw.githubusercontent.com/selva86/datasets/master/diamonds.csv'\n",
    "df = pd.read_csv(filepath).iloc[:, [0,4,6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[11].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mahalanobis_distance(df.iloc[10].values, df.iloc[11].values, df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
